Number of Training Datapoints : 533746
Number of Validation Datapoints : 28092

Training Logs

Step: 1
loss: 32.2865
grad_norm: 133.56082153320312
learning_rate: 0.0019999600319744204
epoch: 5.995203836930456e-05

Step: 500
loss: 0.2472
grad_norm: 0.09802735596895218
learning_rate: 0.001980015987210232
epoch: 0.02997601918465228

Step: 1000
loss: 0.0625
grad_norm: 0.10048986971378326
learning_rate: 0.0019600319744204637
epoch: 0.05995203836930456

Step: 1500
loss: 0.0525
grad_norm: 0.09207140654325485
learning_rate: 0.0019400479616306955
epoch: 0.08992805755395683

Step: 2000
loss: 0.0452
grad_norm: 0.08330415934324265
learning_rate: 0.0019200639488409273
epoch: 0.11990407673860912

Step: 2500
loss: 0.0404
grad_norm: 0.06568878889083862
learning_rate: 0.0019000799360511592
epoch: 0.1498800959232614

Step: 3000
loss: 0.0378
grad_norm: 0.08702881634235382
learning_rate: 0.001880095923261391
epoch: 0.17985611510791366

Step: 3500
loss: 0.0361
grad_norm: 0.05382658541202545
learning_rate: 0.0018601119104716226
epoch: 0.20983213429256595

Step: 4000
loss: 0.0338
grad_norm: 0.047595102339982986
learning_rate: 0.0018401278976818544
epoch: 0.23980815347721823

Step: 4500
loss: 0.0324
grad_norm: 0.06786368042230606
learning_rate: 0.0018201438848920864
epoch: 0.2697841726618705

Step: 5000
loss: 0.0309
grad_norm: 0.06417783349752426
learning_rate: 0.0018001598721023183
epoch: 0.2997601918465228

Step: 5500
loss: 0.0695
grad_norm: 0.10075461864471436
learning_rate: 0.00178017585931255
epoch: 0.32973621103117506

Step: 6000
loss: 0.0307
grad_norm: 0.06657648831605911
learning_rate: 0.0017601918465227817
epoch: 0.3597122302158273

Step: 6500
loss: 0.0285
grad_norm: 0.05853499844670296
learning_rate: 0.0017402078337330135
epoch: 0.38968824940047964

Step: 7000
loss: 0.0272
grad_norm: 0.06408606469631195
learning_rate: 0.0017202238209432456
epoch: 0.4196642685851319

Step: 7500
loss: 0.0264
grad_norm: 0.07411783933639526
learning_rate: 0.0017002398081534774
epoch: 0.44964028776978415

Step: 8000
loss: 0.0259
grad_norm: 0.04570579156279564
learning_rate: 0.001680255795363709
epoch: 0.47961630695443647

Step: 8500
loss: 0.0249
grad_norm: 0.05905548855662346
learning_rate: 0.0016602717825739408
epoch: 0.5095923261390888

Step: 9000
loss: 0.0241
grad_norm: 0.06505037099123001
learning_rate: 0.0016402877697841726
epoch: 0.539568345323741

Step: 9500
loss: 0.0244
grad_norm: 0.07998622208833694
learning_rate: 0.0016203037569944047
epoch: 0.5695443645083933

Step: 10000
loss: 0.0239
grad_norm: 0.06583697348833084
learning_rate: 0.0016003197442046363
epoch: 0.5995203836930456

Step: 10500
loss: 0.0232
grad_norm: 0.04881470277905464
learning_rate: 0.0015803357314148681
epoch: 0.6294964028776978

Step: 11000
loss: 0.0229
grad_norm: 0.057532452046871185
learning_rate: 0.0015603517186251
epoch: 0.6594724220623501

Step: 11500
loss: 0.0237
grad_norm: 0.09548647701740265
learning_rate: 0.0015403677058353318
epoch: 0.6894484412470024

Step: 12000
loss: 0.0227
grad_norm: 0.05268407240509987
learning_rate: 0.0015203836930455636
epoch: 0.7194244604316546

Step: 12500
loss: 0.0216
grad_norm: 0.043151140213012695
learning_rate: 0.0015003996802557954
epoch: 0.749400479616307

Step: 13000
loss: 0.0214
grad_norm: 0.037259358912706375
learning_rate: 0.0014804156674660272
epoch: 0.7793764988009593

Step: 13500
loss: 0.0214
grad_norm: 0.0704680010676384
learning_rate: 0.001460431654676259
epoch: 0.8093525179856115

Step: 14000
loss: 0.0206
grad_norm: 0.0826869085431099
learning_rate: 0.0014404476418864909
epoch: 0.8393285371702638

Step: 14500
loss: 0.02
grad_norm: 0.06275249272584915
learning_rate: 0.0014204636290967225
epoch: 0.8693045563549161

Step: 15000
loss: 0.0201
grad_norm: 0.051824651658535004
learning_rate: 0.0014004796163069545
epoch: 0.8992805755395683

Step: 15500
loss: 0.0201
grad_norm: 0.04567379504442215
learning_rate: 0.0013804956035171863
epoch: 0.9292565947242206

Step: 16000
loss: 0.0194
grad_norm: 0.04073363542556763
learning_rate: 0.0013605115907274182
epoch: 0.9592326139088729

Step: 16500
loss: 0.0196
grad_norm: 0.04459104686975479
learning_rate: 0.00134052757793765
epoch: 0.9892086330935251

Step: 16680
eval_loss: 0.015822099521756172
eval_runtime: 108.0239
eval_samples_per_second: 260.053
eval_steps_per_second: 8.128
epoch: 1.0

Step: 17000
loss: 0.0179
grad_norm: 0.0515451543033123
learning_rate: 0.0013205435651478816
epoch: 1.0191846522781776

Step: 17500
loss: 0.017
grad_norm: 0.0613674595952034
learning_rate: 0.0013005595523581134
epoch: 1.0491606714628297

Step: 18000
loss: 0.0171
grad_norm: 0.06028446555137634
learning_rate: 0.0012805755395683455
epoch: 1.079136690647482

Step: 18500
loss: 0.0171
grad_norm: 0.040020592510700226
learning_rate: 0.0012605915267785773
epoch: 1.1091127098321343

Step: 19000
loss: 0.0169
grad_norm: 0.04219629615545273
learning_rate: 0.0012406075139888089
epoch: 1.1390887290167866

Step: 19500
loss: 0.0167
grad_norm: 0.03952554613351822
learning_rate: 0.0012206235011990407
epoch: 1.169064748201439

Step: 20000
loss: 0.0169
grad_norm: 0.025489525869488716
learning_rate: 0.0012006394884092725
epoch: 1.1990407673860912

Step: 20500
loss: 0.0167
grad_norm: 0.03509526327252388
learning_rate: 0.0011806554756195046
epoch: 1.2290167865707433

Step: 21000
loss: 0.0165
grad_norm: 0.04052428901195526
learning_rate: 0.0011606714628297362
epoch: 1.2589928057553956

Step: 21500
loss: 0.016
grad_norm: 0.03644609451293945
learning_rate: 0.001140687450039968
epoch: 1.288968824940048

Step: 22000
loss: 0.0161
grad_norm: 0.03129676356911659
learning_rate: 0.0011207034372501998
epoch: 1.3189448441247003

Step: 22500
loss: 0.0161
grad_norm: 0.038409244269132614
learning_rate: 0.0011007194244604316
epoch: 1.3489208633093526

Step: 23000
loss: 0.0154
grad_norm: 0.04319159314036369
learning_rate: 0.0010807354116706637
epoch: 1.3788968824940047

Step: 23500
loss: 0.0155
grad_norm: 0.04123576357960701
learning_rate: 0.0010607513988808953
epoch: 1.4088729016786572

Step: 24000
loss: 0.0153
grad_norm: 0.038502853363752365
learning_rate: 0.0010407673860911271
epoch: 1.4388489208633093

Step: 24500
loss: 0.0152
grad_norm: 0.04366852343082428
learning_rate: 0.001020783373301359
epoch: 1.4688249400479616

Step: 25000
loss: 0.0152
grad_norm: 0.031966280192136765
learning_rate: 0.0010007993605115908
epoch: 1.498800959232614

Step: 25500
loss: 0.0148
grad_norm: 0.028178546577692032
learning_rate: 0.0009808153477218226
epoch: 1.5287769784172662

Step: 26000
loss: 0.0152
grad_norm: 0.054858673363924026
learning_rate: 0.0009608313349320543
epoch: 1.5587529976019185

Step: 26500
loss: 0.0151
grad_norm: 0.04448378458619118
learning_rate: 0.0009408473221422862
epoch: 1.5887290167865706

Step: 27000
loss: 0.0147
grad_norm: 0.07250290364027023
learning_rate: 0.0009208633093525181
epoch: 1.6187050359712232

Step: 27500
loss: 0.0144
grad_norm: 0.025055719539523125
learning_rate: 0.0009008792965627498
epoch: 1.6486810551558753

Step: 28000
loss: 0.0141
grad_norm: 0.029569782316684723
learning_rate: 0.0008808952837729817
epoch: 1.6786570743405276

Step: 28500
loss: 0.0144
grad_norm: 0.03462103754281998
learning_rate: 0.0008609112709832134
epoch: 1.70863309352518

Step: 29000
loss: 0.014
grad_norm: 0.03449259698390961
learning_rate: 0.0008409272581934452
epoch: 1.738609112709832

Step: 29500
loss: 0.0139
grad_norm: 0.030346529558300972
learning_rate: 0.0008209432454036771
epoch: 1.7685851318944845

Step: 30000
loss: 0.0138
grad_norm: 0.041792336851358414
learning_rate: 0.0008009592326139089
epoch: 1.7985611510791366

Step: 30500
loss: 0.0142
grad_norm: 0.03924937546253204
learning_rate: 0.0007809752198241407
epoch: 1.828537170263789

Step: 31000
loss: 0.0133
grad_norm: 0.03704886510968208
learning_rate: 0.0007609912070343725
epoch: 1.8585131894484412

Step: 31500
loss: 0.0138
grad_norm: 0.0330641083419323
learning_rate: 0.0007410071942446042
epoch: 1.8884892086330936

Step: 32000
loss: 0.0133
grad_norm: 0.027011968195438385
learning_rate: 0.0007210231814548362
epoch: 1.9184652278177459

Step: 32500
loss: 0.0129
grad_norm: 0.04598906636238098
learning_rate: 0.000701039168665068
epoch: 1.948441247002398

Step: 33000
loss: 0.0132
grad_norm: 0.03236927092075348
learning_rate: 0.0006810551558752997
epoch: 1.9784172661870505

Step: 33360
eval_loss: 0.01138672698289156
eval_runtime: 112.7609
eval_samples_per_second: 249.129
eval_steps_per_second: 7.786
epoch: 2.0

Step: 33500
loss: 0.0122
grad_norm: 0.04423745349049568
learning_rate: 0.0006610711430855317
epoch: 2.0083932853717026

Step: 34000
loss: 0.0111
grad_norm: 0.026661871001124382
learning_rate: 0.0006410871302957634
epoch: 2.038369304556355

Step: 34500
loss: 0.0109
grad_norm: 0.03606482222676277
learning_rate: 0.0006211031175059952
epoch: 2.068345323741007

Step: 35000
loss: 0.011
grad_norm: 0.04610248655080795
learning_rate: 0.000601119104716227
epoch: 2.0983213429256593

Step: 35500
loss: 0.0108
grad_norm: 0.03426671773195267
learning_rate: 0.0005811350919264588
epoch: 2.128297362110312

Step: 36000
loss: 0.0109
grad_norm: 0.026669511571526527
learning_rate: 0.0005611510791366907
epoch: 2.158273381294964

Step: 36500
loss: 0.0107
grad_norm: 0.03808915242552757
learning_rate: 0.0005411670663469225
epoch: 2.1882494004796165

Step: 37000
loss: 0.0106
grad_norm: 0.04195452108979225
learning_rate: 0.0005211830535571543
epoch: 2.2182254196642686

Step: 37500
loss: 0.0105
grad_norm: 0.03007865883409977
learning_rate: 0.0005011990407673861
epoch: 2.2482014388489207

Step: 38000
loss: 0.0108
grad_norm: 0.023621629923582077
learning_rate: 0.0004812150279776179
epoch: 2.278177458033573

Step: 38500
loss: 0.0104
grad_norm: 0.025015413761138916
learning_rate: 0.0004612310151878497
epoch: 2.3081534772182253

Step: 39000
loss: 0.0107
grad_norm: 0.046451907604932785
learning_rate: 0.00044124700239808154
epoch: 2.338129496402878

Step: 39500
loss: 0.0104
grad_norm: 0.026856957003474236
learning_rate: 0.00042126298960831337
epoch: 2.36810551558753

Step: 40000
loss: 0.0101
grad_norm: 0.04089336842298508
learning_rate: 0.0004012789768185452
epoch: 2.3980815347721824

Step: 40500
loss: 0.01
grad_norm: 0.03709433972835541
learning_rate: 0.000381294964028777
epoch: 2.4280575539568345

Step: 41000
loss: 0.0101
grad_norm: 0.033448271453380585
learning_rate: 0.00036131095123900883
epoch: 2.4580335731414866

Step: 41500
loss: 0.01
grad_norm: 0.024275926873087883
learning_rate: 0.0003413269384492406
epoch: 2.488009592326139

Step: 42000
loss: 0.0099
grad_norm: 0.02420758083462715
learning_rate: 0.0003213429256594724
epoch: 2.5179856115107913

Step: 42500
loss: 0.0102
grad_norm: 0.031202388927340508
learning_rate: 0.00030135891286970425
epoch: 2.547961630695444

Step: 43000
loss: 0.0097
grad_norm: 0.03114505112171173
learning_rate: 0.0002813749000799361
epoch: 2.577937649880096

Step: 43500
loss: 0.0096
grad_norm: 0.02412489987909794
learning_rate: 0.00026139088729016784
epoch: 2.6079136690647484

Step: 44000
loss: 0.0094
grad_norm: 0.03972765430808067
learning_rate: 0.0002414068745003997
epoch: 2.6378896882494005

Step: 44500
loss: 0.0094
grad_norm: 0.043474458158016205
learning_rate: 0.0002214228617106315
epoch: 2.6678657074340526

Step: 45000
loss: 0.0096
grad_norm: 0.0431402288377285
learning_rate: 0.0002014388489208633
epoch: 2.697841726618705

Step: 45500
loss: 0.0097
grad_norm: 0.03130102902650833
learning_rate: 0.0001814548361310951
epoch: 2.7278177458033572

Step: 46000
loss: 0.0092
grad_norm: 0.028367076069116592
learning_rate: 0.00016147082334132696
epoch: 2.7577937649880093

Step: 46500
loss: 0.0094
grad_norm: 0.018499113619327545
learning_rate: 0.00014148681055155875
epoch: 2.787769784172662

Step: 47000
loss: 0.0094
grad_norm: 0.02942466363310814
learning_rate: 0.00012150279776179058
epoch: 2.8177458033573144

Step: 47500
loss: 0.0094
grad_norm: 0.0319833904504776
learning_rate: 0.00010151878497202239
epoch: 2.8477218225419665

Step: 48000
loss: 0.0091
grad_norm: 0.028677331283688545
learning_rate: 8.153477218225421e-05
epoch: 2.8776978417266186

Step: 48500
loss: 0.0092
grad_norm: 0.029984593391418457
learning_rate: 6.155075939248602e-05
epoch: 2.907673860911271

Step: 49000
loss: 0.0092
grad_norm: 0.039719972759485245
learning_rate: 4.156674660271783e-05
epoch: 2.937649880095923

Step: 49500
loss: 0.009
grad_norm: 0.02415909618139267
learning_rate: 2.1582733812949642e-05
epoch: 2.9676258992805753

Step: 50000
loss: 0.009
grad_norm: 0.022048192098736763
learning_rate: 1.5987210231814547e-06
epoch: 2.997601918465228

Step: 50040
eval_loss: 0.009198976680636406
eval_runtime: 109.855
eval_samples_per_second: 255.719
eval_steps_per_second: 7.992
epoch: 3.0

Step: 50040
train_runtime: 15865.2957
train_samples_per_second: 100.927
train_steps_per_second: 3.154
total_flos: 7.441373499280589e+16
train_loss: 0.02108749030626935
epoch: 3.0

